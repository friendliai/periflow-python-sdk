# The name of experiment
experiment: huggingface-clm

# The name of job
name: clm-job

# The name of vm type
vm: azure-16gb-v100-2g-eastus-spot

# The number of GPU devices
num_devices: 2

# Configure your job!
job_setting:
  type: custom

  # Docker config
  docker:
    # Docker image you want to use in the job
    image: friendliai/periflow:sdk
    # Bash shell command to run the job
    command: >
      cd /workspace/huggingface && pip install -r requirements.txt && torchrun --nnodes $NUM_NODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port 6000 --nproc_per_node $NPROC_PER_NODE run_clm.py \
        --model_name_or_path gpt2 \
        --dataset_name wikitext \
        --dataset_config_name wikitext-2-raw-v1 \
        --dataset_saved_dir /workspace/data \
        --per_device_train_batch_size 1 \
        --per_device_eval_batch_size 1 \
        --do_train \
        --do_eval \
        --output_dir /workspace/huggingface/test-clm
  # Path to mount your workspace volume
  workspace:
    mount_path: /workspace

# Checkpoint config
checkpoint:
  # # Input checkpoint
  # input:
  #   id:
  #   mount_path: /workspace/ckpt
  # Path to output checkpoint
  output_checkpoint_dir: /workspace/ckpt

# Dataset config
data:
  name: st-wikitext
  mount_path: /workspace/data
