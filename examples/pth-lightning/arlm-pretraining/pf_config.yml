# The name of experiment
experiment: lightning-arlm

# The name of job
name: arlm-run-gpt-j

# The name of vm type
vm: azure-80gb-a100-2g-eastus2

# The number of GPU devices
num_devices: 2

# Configure your job!
job_setting:
  type: custom

  # Docker config
  docker:
    # Docker image you want to use in the job
    image: friendliai/periflow:sdk
    # Bash shell command to run the job
    command: >
      cd /workspace/arlm_pretraining && pip install -r requirements.txt && torchrun --nnodes $NUM_NODES --node_rank $NODE_RANK --master_addr $MASTER_ADDR --master_port 6000 --nproc_per_node $NPROC_PER_NODE main.py \
        --dataset-name wikitext \
        --dataset-config-name wikitext-2-raw-v1 \
        --model-name EleutherAI/gpt-j-6B \
        --tokenizer-name EleutherAI/gpt-j-6B \
        --checkpoint-dir /workspace/ckpt \
        --cache-dir /workspace/data/wikitext \
        --num-epochs 5 \
        --batch-size 1 \
        --num-gpus 2 \
        --log-interval 10
  # Path to mount your workspace volume
  workspace:
    mount_path: /workspace

# Checkpoint config
checkpoint:
  # Input checkpoint config: Specify checkpoint ID and add `--load-from-checkpoint` argument
  # input:
  #   id: 25ac0c73-d22a-4bda-8433-58cf7c81ed8a
  #   mount_path: /workspace/ckpt
  # Path to output checkpoint
  output_checkpoint_dir: /workspace/ckpt

# Dataset config
data:
  name: wikitext
  mount_path: /workspace/data
